{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacdc922",
   "metadata": {},
   "source": [
    "**Zero-Shot Prompting**\n",
    "\n",
    "\tâ€¢ What it is: You give the model only the instructions, context, and question. No examples.\n",
    "\tâ€¢ When to use:\n",
    "\tâ€¢ When you donâ€™t have sample Q&A pairs.\n",
    "\tâ€¢ For general-purpose answering.\n",
    "\tâ€¢ When you want the model to reason freely but still stay grounded in the context.\n",
    "\tâ€¢ Trade-off: Faster and simpler, but sometimes answers may not follow the exact style or format you want.\n",
    "\n",
    "Example:\n",
    "\n",
    "Context about Transformers â†’ Ask: â€œHow do they handle long-range dependencies?â€\n",
    "\n",
    "The model figures out the answer directly from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a013e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Gen_AI\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95b1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(context:str , query:str)-> str:\n",
    "    llm = Groq(\n",
    "        model = \"llama-3.1-8b-instant\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    template_str = (\n",
    "        \"You are an expert AI assistant.\\n\"\n",
    "        \"Use ONLY use the provided context to answer the user's question. \"\n",
    "        \"If the context is insufficient or does not mention the answer, reply exactly: \"\n",
    "        \"'Not enough information.'\\n\\n\"\n",
    "        \"Context:\\n{context_str}\\n\\n\"\n",
    "        \"User Question: {query_str}\\n\\n\"\n",
    "        \"Answering Rules:\\n\"\n",
    "        \"1) Be concise and precise (3â€“6 sentences, unless the question requires more).\\n\"\n",
    "        \"2) Use bullet points for lists.\\n\"\n",
    "        \"3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\\n\\n\"\n",
    "        \"Final Answer:\"\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(template_str)\n",
    "    filled_prompt = prompt.format(\n",
    "        context_str = context,\n",
    "        query_str = query\n",
    "    )\n",
    "    response = llm.complete(filled_prompt)\n",
    "    output = response.text\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9becdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers handle long-range dependencies using a self-attention mechanism. This mechanism allows each token to attend to all other tokens in the sequence, enabling the model to capture long-range relationships.\n",
      "\n",
      "Key features of the self-attention mechanism:\n",
      "\n",
      "* Each token attends to all other tokens in the sequence.\n",
      "* This allows the model to capture long-range dependencies without recurrence.\n",
      "* The attention weights are calculated based on the similarity between tokens.\n",
      "\n",
      "Sources:\n",
      "Transformers use a self-attention mechanism that allows each token to attend to all other tokens in the sequence. This helps capture long-range dependencies without recurrence.\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot: No examples, just context + query\n",
    "context_text = (\n",
    "    \"Transformers use a self-attention mechanism that allows each token \"\n",
    "    \"to attend to all other tokens in the sequence. This helps capture \"\n",
    "    \"long-range dependencies without recurrence.\"\n",
    ")\n",
    "\n",
    "query_text = \"How do Transformers handle long-range dependencies?\"\n",
    "\n",
    "ans0 = run_llm(context=context_text, query=query_text)\n",
    "\n",
    "print(ans0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541fda5",
   "metadata": {},
   "source": [
    "**Few-Shot Prompting**\n",
    "\n",
    "\tâ€¢ What it is: You give the model a few examples of context â†’ question â†’ answer, then add your new context + question.\n",
    "\tâ€¢ When to use:\n",
    "\tâ€¢ When you want the model to mimic a specific style (tone, structure, format).\n",
    "\tâ€¢ If you want consistent answers (e.g., always ending with â€œSources: â€¦â€).\n",
    "\tâ€¢ Good for teaching the model special formatting rules or domain-specific style.\n",
    "\tâ€¢ Trade-off: More control, but you must prepare good examples, and the prompt can get longer.\n",
    "\n",
    "Example:\n",
    "\n",
    "Show 2â€“3 sample Q&A pairs â†’ Then ask a new question with new context.\n",
    "The model copies the answering style from your examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "def run_llm_fewshot(context: str, query: str, examples: List[Dict[str, str]]) -> str:\n",
    "    llm = Groq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "\n",
    "    examples_str = \"\\n\".join(\n",
    "        f\"Example {i+1}:\\nContext:\\n{ex.get('context','')}\\n\"\n",
    "        f\"Question: {ex.get('question','')}\\n\"\n",
    "        f\"Answer: {ex.get('answer','')}\\n\"\n",
    "        for i, ex in enumerate(examples)\n",
    "    )\n",
    "\n",
    "    template_str = (\n",
    "        \"You are an expert AI assistant.\\n\"\n",
    "        \"Use ONLY the provided context to answer the user's question. \"\n",
    "        \"If the context is insufficient or does not mention the answer, reply exactly: \"\n",
    "        \"'Not enough information.'\\n\\n\"\n",
    "        \"Follow the style and reasoning illustrated by the examples.\\n\\n\"\n",
    "        \"Examples:\\n{examples_str}\\n\"\n",
    "        \"--- End of Examples ---\\n\\n\"\n",
    "        \"Context:\\n{context_str}\\n\\n\"\n",
    "        \"User Question: {query_str}\\n\\n\"\n",
    "        \"Answering Rules:\\n\"\n",
    "        \"1) Be concise and precise (3â€“6 sentences, unless the question requires more).\\n\"\n",
    "        \"2) Use bullet points for lists.\\n\"\n",
    "        \"3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\\n\\n\"\n",
    "        \"Final Answer:\"\n",
    "    )\n",
    "    prompt = PromptTemplate(template_str).format(\n",
    "        examples_str=examples_str, context_str=context, query_str=query\n",
    "    )\n",
    "    return llm.complete(prompt=prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6a8fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the attention mechanism, softmax is used to normalize the similarity scores between queries and keys. This process produces attention weights that represent the relative importance of each key with respect to the query. The softmax function ensures that the attention weights add up to 1, allowing the model to focus on the most relevant keys. Key benefits of using softmax include:\n",
      "* Normalizing the similarity scores\n",
      "* Producing a probability distribution over the keys\n",
      "Sources: attention_mechanism.pdf\n"
     ]
    }
   ],
   "source": [
    "shots = [\n",
    "    {\n",
    "        \"context\": \"Positional encodings inject order information into sequences.\",\n",
    "        \"question\": \"Why are positional encodings needed?\",\n",
    "        \"answer\": (\n",
    "            \"They give the model a sense of word order.\\n\"\n",
    "            \"- Without them, the model treats tokens as a bag of words.\\n\"\n",
    "            \"- Encodings ensure the sequence structure is preserved.\\n\"\n",
    "            \"Sources: lecture_notes.txt\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Multi-head attention projects queries, keys, and values into multiple subspaces.\",\n",
    "        \"question\": \"What is the benefit of multi-head attention?\",\n",
    "        \"answer\": (\n",
    "            \"It lets the model learn from different representation subspaces.\\n\"\n",
    "            \"- Captures diverse relationships.\\n\"\n",
    "            \"- Improves contextual understanding.\\n\"\n",
    "            \"Sources: attention_paper.pdf\"\n",
    "        )\n",
    "    },\n",
    "]\n",
    "\n",
    "context_text = (\n",
    "    \"Context from attention_mechanism.pdf\"\n",
    "    \"In the attention mechanism, softmax is used on the similarity scores \"\n",
    "    \"between queries and keys to produce attention weights.\"\n",
    ")\n",
    "\n",
    "query_text = \"what does softmax do in attention\"\n",
    "\n",
    "answer = run_llm_fewshot(\n",
    "    context = context_text,\n",
    "    query = query_text,\n",
    "    examples = shots\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec9645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcb6c403",
   "metadata": {},
   "source": [
    "**ðŸ‘‰ Rule of thumb:**\n",
    "\n",
    "\tâ€¢ Use Zero-Shot for quick, flexible answers.\n",
    "\tâ€¢ Use Few-Shot when consistency, formatting, or special style matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620af39",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
