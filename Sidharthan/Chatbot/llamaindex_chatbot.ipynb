{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa579ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.prompts import PromptTemplate , ChatMessage, MessageRole\n",
    "#from llama_index.core.llms import ChatMessage , MessageRole\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb30ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6cb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(\n",
    "    model = \"llama-3.1-8b-instant\", \n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd412595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input):\n",
    "    history = [\n",
    "        ChatMessage(role= MessageRole.SYSTEM, content=\"You are a helpful assistant.Be concise and accurate.\"),\n",
    "        ChatMessage(role= MessageRole.USER, content=user_input)\n",
    "    ]\n",
    "\n",
    "    response = llm.chat(history)\n",
    "    answer = response.message.content\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72b8246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Deep learning is a subset of machine learning that involves the use of artificial neural networks with multiple layers to analyze and interpret data. These neural networks are inspired by the structure and function of the human brain, with each layer processing and transforming the input data in a hierarchical manner.\n",
      "\n",
      "Deep learning models are trained on large datasets and can learn complex patterns and relationships between data points. They are particularly effective in image and speech recognition, natural language processing, and other applications where data is high-dimensional and complex.\n",
      "\n",
      "Some key characteristics of deep learning include:\n",
      "\n",
      "1. **Multiple layers**: Deep learning models have multiple layers of interconnected nodes (neurons) that process and transform the input data.\n",
      "2. **Non-linear transformations**: Each layer applies a non-linear transformation to the input data, allowing the model to learn complex relationships between data points.\n",
      "3. **Large datasets**: Deep learning models require large datasets to train and learn from.\n",
      "4. **Automatic feature learning**: Deep learning models can automatically learn relevant features from the data, rather than relying on hand-engineered features.\n",
      "\n",
      "Some common applications of deep learning include:\n",
      "\n",
      "1. **Image recognition**: Deep learning models can recognize objects, scenes, and activities in images.\n",
      "2. **Speech recognition**: Deep learning models can recognize spoken words and phrases.\n",
      "3. **Natural language processing**: Deep learning models can understand and generate human language.\n",
      "4. **Robotics**: Deep learning models can control robots and other autonomous systems.\n",
      "\n",
      "Some popular deep learning techniques include:\n",
      "\n",
      "1. **Convolutional neural networks (CNNs)**: Used for image recognition and other computer vision tasks.\n",
      "2. **Recurrent neural networks (RNNs)**: Used for speech recognition and other sequential data tasks.\n",
      "3. **Long short-term memory (LSTM) networks**: A type of RNN that is particularly effective for sequential data tasks.\n",
      "4. **Generative adversarial networks (GANs)**: Used for generating new data samples that are similar to existing data.\n",
      "Chatbot: **Transformer Architecture Overview**\n",
      "\n",
      "The Transformer architecture is a type of neural network designed for sequence-to-sequence tasks, such as machine translation, text summarization, and question answering. It was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\n",
      "\n",
      "**Key Components**\n",
      "\n",
      "1. **Encoder**: The encoder takes in a sequence of tokens (e.g., words or characters) and outputs a continuous representation of the input sequence.\n",
      "2. **Decoder**: The decoder takes in the output of the encoder and generates a sequence of tokens.\n",
      "3. **Self-Attention Mechanism**: The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance.\n",
      "4. **Feed-Forward Network (FFN)**: The FFN is a fully connected neural network that transforms the output of the self-attention mechanism.\n",
      "\n",
      "**Transformer Architecture Diagram**\n",
      "\n",
      "```\n",
      "          +---------------+\n",
      "          |  Input Embed  |\n",
      "          +---------------+\n",
      "                  |\n",
      "                  |\n",
      "                  v\n",
      "+---------------------------------------+\n",
      "|  Encoder (Self-Attention, FFN)  |  |\n",
      "+---------------------------------------+\n",
      "|  Encoder Output  |  |\n",
      "+---------------------------------------+\n",
      "                  |\n",
      "                  |\n",
      "                  v\n",
      "+---------------------------------------+\n",
      "|  Decoder (Self-Attention, FFN)  |  |\n",
      "+---------------------------------------+\n",
      "|  Decoder Output  |  |\n",
      "+---------------------------------------+\n",
      "                  |\n",
      "                  |\n",
      "                  v\n",
      "          +---------------+\n",
      "          |  Output Embed  |\n",
      "          +---------------+\n",
      "```\n",
      "\n",
      "**Code Snippet (PyTorch)**\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "class Transformer(nn.Module):\n",
      "    def __init__(self, num_encoder_layers, num_decoder_layers, hidden_size, num_heads, dropout):\n",
      "        super(Transformer, self).__init__()\n",
      "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout)\n",
      "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout)\n",
      "        self.encoder_norm = nn.LayerNorm(hidden_size)\n",
      "        self.decoder_norm = nn.LayerNorm(hidden_size)\n",
      "        self.generator = nn.Linear(hidden_size, hidden_size)\n",
      "\n",
      "    def forward(self, src, tgt):\n",
      "        src = self.encoder(src)\n",
      "        tgt = self.decoder(tgt, src)\n",
      "        tgt = self.decoder_norm(tgt)\n",
      "        output = self.generator(tgt)\n",
      "        return output\n",
      "\n",
      "class TransformerEncoder(nn.Module):\n",
      "    def __init__(self, num_layers, hidden_size, num_heads, dropout):\n",
      "        super(TransformerEncoder, self).__init__()\n",
      "        self.layers = nn.ModuleList([nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout) for _ in range(num_layers)])\n",
      "\n",
      "    def forward(self, src):\n",
      "        for layer in self.layers:\n",
      "            src = layer(src)\n",
      "        return src\n",
      "\n",
      "class TransformerDecoder(nn.Module):\n",
      "    def __init__(self, num_layers, hidden_size, num_heads, dropout):\n",
      "        super(TransformerDecoder, self).__init__()\n",
      "        self.layers = nn.ModuleList([nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout) for _ in range(num_layers)])\n",
      "\n",
      "    def forward(self, tgt, src):\n",
      "        for layer in self.layers:\n",
      "            tgt = layer(tgt, src)\n",
      "        return tgt\n",
      "\n",
      "# Example usage\n",
      "src = torch.randn(1, 10, 512)  # batch size, sequence length, hidden size\n",
      "tgt = torch.randn(1, 10, 512)  # batch size, sequence length, hidden size\n",
      "\n",
      "model = Transformer(num_encoder_layers=6, num_decoder_layers=6, hidden_size=512, num_heads=8, dropout=0.1)\n",
      "output = model(src, tgt)\n",
      "print(output.shape)\n",
      "```\n",
      "\n",
      "This code snippet defines a basic Transformer architecture with an encoder and a decoder. The `Transformer` class takes in the input sequence `src` and the target sequence `tgt` and outputs a continuous representation of the input sequence. The `TransformerEncoder` and `TransformerDecoder` classes are used to define the encoder and decoder layers, respectively. The example usage demonstrates how to create a Transformer model and pass in input sequences to generate output.\n",
      "Chatbot: **Transformer Architecture Overview**\n",
      "\n",
      "The Transformer architecture is a neural network design introduced in 2017 by Vaswani et al. It revolutionized the field of natural language processing (NLP) and has since been widely adopted in various applications, including machine translation, text classification, and question answering.\n",
      "\n",
      "**Key Components**\n",
      "\n",
      "1. **Self-Attention Mechanism**: The Transformer's core component is the self-attention mechanism, which allows the model to weigh the importance of different input elements relative to each other. This is achieved through a dot product of the query and key vectors, followed by a softmax function.\n",
      "2. **Encoder-Decoder Structure**: The Transformer consists of an encoder and a decoder. The encoder takes in a sequence of tokens (e.g., words or characters) and outputs a sequence of vectors. The decoder takes in the output of the encoder and generates a sequence of tokens.\n",
      "3. **Layer Normalization**: Each layer in the Transformer uses layer normalization to normalize the input and output vectors.\n",
      "4. **Positional Encoding**: To preserve the order of the input sequence, the Transformer uses positional encoding, which adds a sinusoidal function to the input vectors based on their position.\n",
      "\n",
      "**Architecture Diagram**\n",
      "\n",
      "Here's a high-level overview of the Transformer architecture:\n",
      "\n",
      "```\n",
      "Encoder:\n",
      "  - Input Embedding\n",
      "  - Self-Attention (Layer 1)\n",
      "  - Feed Forward Network (Layer 1)\n",
      "  - Layer Normalization (Layer 1)\n",
      "  - Repeat (Layers 2-6)\n",
      "  - Output Embedding\n",
      "\n",
      "Decoder:\n",
      "  - Input Embedding\n",
      "  - Self-Attention (Layer 1)\n",
      "  - Encoder-Decoder Attention (Layer 1)\n",
      "  - Feed Forward Network (Layer 1)\n",
      "  - Layer Normalization (Layer 1)\n",
      "  - Repeat (Layers 2-6)\n",
      "  - Output Embedding\n",
      "```\n",
      "\n",
      "**How it Works**\n",
      "\n",
      "1. The input sequence is embedded into a vector space using an embedding layer.\n",
      "2. The embedded sequence is passed through the encoder, which consists of multiple self-attention layers and feed forward networks.\n",
      "3. The output of the encoder is passed through the decoder, which consists of multiple self-attention layers, encoder-decoder attention layers, and feed forward networks.\n",
      "4. The final output is generated by the decoder, which is a sequence of tokens.\n",
      "\n",
      "**Advantages**\n",
      "\n",
      "1. **Parallelization**: The Transformer can be parallelized more easily than recurrent neural networks (RNNs), making it faster to train.\n",
      "2. **Scalability**: The Transformer can handle longer input sequences than RNNs.\n",
      "3. **Flexibility**: The Transformer can be used for a wide range of NLP tasks, including machine translation, text classification, and question answering.\n",
      "\n",
      "**Disadvantages**\n",
      "\n",
      "1. **Computational Complexity**: The Transformer requires more computational resources than RNNs.\n",
      "2. **Memory Requirements**: The Transformer requires more memory to store the input sequence and the output sequence.\n",
      "\n",
      "Overall, the Transformer architecture has revolutionized the field of NLP and has become a widely adopted design for various applications.\n",
      "Exiting the chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting the chatbot. Goodbye!\")\n",
    "        break\n",
    "    answer = chat(user_input)\n",
    "    print(\"Chatbot:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df10d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
