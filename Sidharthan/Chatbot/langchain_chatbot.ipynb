{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa606164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29b5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88366dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3650e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4683588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input):\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\" , \"You are a helpful assistant . Be concise and accurate\"),\n",
    "            (\"user\" , \"{user_input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | parser\n",
    "    response = chain.invoke({\"user_input\": user_input})\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab87ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: **Self-Attention Mechanism**\n",
      "\n",
      "The self-attention mechanism is a key component of transformer models, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. It allows the model to weigh the importance of different input elements relative to each other.\n",
      "\n",
      "**How it works**\n",
      "\n",
      "Given a sequence of vectors `x = [x1, x2, ..., xn]`, the self-attention mechanism computes a weighted sum of these vectors, where the weights are learned based on the similarity between the vectors.\n",
      "\n",
      "Here's a step-by-step breakdown:\n",
      "\n",
      "1. **Query (Q)**: Compute a query vector `q` for each input vector `xi`.\n",
      "2. **Key (K)**: Compute a key vector `k` for each input vector `xi`.\n",
      "3. **Value (V)**: Compute a value vector `v` for each input vector `xi`.\n",
      "4. **Attention weights**: Compute the attention weights `a` by taking the dot product of `q` and `k`, and applying a softmax function.\n",
      "5. **Weighted sum**: Compute the weighted sum of the value vectors `v` using the attention weights `a`.\n",
      "\n",
      "**Mathematical formulation**\n",
      "\n",
      "Let `x` be the input sequence, `q`, `k`, and `v` be the query, key, and value vectors, respectively. The self-attention mechanism can be formulated as:\n",
      "\n",
      "`a = softmax(q * k^T / sqrt(d))`\n",
      "`o = a * v`\n",
      "\n",
      "where `o` is the output sequence, `d` is the dimensionality of the input vectors, and `*` denotes the dot product.\n",
      "\n",
      "**Code snippet (PyTorch)**\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class SelfAttention(nn.Module):\n",
      "    def __init__(self, num_heads, hidden_size):\n",
      "        super(SelfAttention, self).__init__()\n",
      "        self.num_heads = num_heads\n",
      "        self.hidden_size = hidden_size\n",
      "        self.query_linear = nn.Linear(hidden_size, hidden_size)\n",
      "        self.key_linear = nn.Linear(hidden_size, hidden_size)\n",
      "        self.value_linear = nn.Linear(hidden_size, hidden_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        batch_size, seq_len, hidden_size = x.size()\n",
      "        num_heads = self.num_heads\n",
      "\n",
      "        # Compute query, key, and value vectors\n",
      "        q = self.query_linear(x).view(batch_size, seq_len, num_heads, -1).transpose(1, 2)\n",
      "        k = self.key_linear(x).view(batch_size, seq_len, num_heads, -1).transpose(1, 2)\n",
      "        v = self.value_linear(x).view(batch_size, seq_len, num_heads, -1).transpose(1, 2)\n",
      "\n",
      "        # Compute attention weights\n",
      "        attention_weights = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.hidden_size)\n",
      "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
      "\n",
      "        # Compute weighted sum\n",
      "        output = torch.matmul(attention_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
      "\n",
      "        return output\n",
      "```\n",
      "\n",
      "This code snippet defines a PyTorch module `SelfAttention` that implements the self-attention mechanism. The `forward` method takes an input tensor `x` and returns the output tensor `o`. The `num_heads` and `hidden_size` parameters control the number of attention heads and the dimensionality of the input vectors, respectively.\n",
      "________________________________________________________________________________\n",
      "Exiting chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = chat(user_input)\n",
    "    print(\"Bot:\", response)\n",
    "    print(\"_\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7548c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
